# encoder-GLAT-NAT
The idea of this project is that if the input at the decoder side is closer to the ground truth, the predicted target sentence will be more accurate. 
The source sentence contains all the semantic information, so if the encoder is used to directly predict the target sentence which is used as the input of the decoder, 
the accuracy of the final sentence predicted by the decoder can be greatly improved.


The model architecture will adopt the current popular Transformer architecture. 
In the training phase, first, the encoder is used to directly predict the target sentence and construct a loss function. 
Since the length of the predicted target sentence is different from the length of the source sentence, 
we use the interpolation method to obtain the hidden states of the predicted target sentence based on the hidden states generated by the encoder. 
Secondly, the target sentence predicted by the encoder is used as the input of the decoder, and the output of the decoder is obtained for loss function optimization. 
The purpose of doing this is equivalent to performing a one-step prediction in the encoder and a refinement in the decoder.


In order to improve the error correction capability of the decoder, 
we borrowed the method of [GLAT](https://arxiv.org/pdf/2008.07905.pdf) to mask the tokens whose prediction by the encoder is not accurate according to a certain ratio, 
and let the decoder predict the tokens of the masked tokens. However, this method does not know which tokens should be masked in the inference stage. 
In order to solve this problem, a discriminator can be used to perform binary classification on the tokens predicted by the encoder to determine whether 
the predicted tokens are correct, but no further attempts were made.
