# Encoder-GLAT-NAT
The idea of this project is that if the input at the decoder side is closer to the ground truth, the predicted target sentence will be more accurate. 
The source sentence contains all the semantic information, so if the encoder is used to directly predict the target sentence which is used as the input of the decoder, 
the accuracy of the final sentence predicted by the decoder can be greatly improved.


The model architecture will adopt the current popular Transformer architecture. 
In the training phase, first, the encoder is used to directly predict the target sentence and construct a loss function. 
Since the length of the predicted target sentence is different from the length of the source sentence, 
we use the interpolation method to obtain the hidden states of the predicted target sentence based on the hidden states generated by the encoder. 
Secondly, the target sentence predicted by the encoder is used as the input of the decoder, and the output of the decoder is obtained for loss function optimization. 
The purpose of doing this is equivalent to performing a one-step prediction in the encoder and a refinement in the decoder.


In order to improve the error correction capability of the decoder, 
we borrowed the method of [GLAT](https://arxiv.org/pdf/2008.07905.pdf) to mask the tokens whose prediction by the encoder is not accurate according to a certain ratio, 
and let the decoder predict the tokens of the masked tokens. However, this method does not know which tokens should be masked in the inference stage. 
In order to solve this problem, a discriminator can be used to perform binary classification on the tokens predicted by the encoder to determine whether 
the predicted tokens are correct, but no further attempts were made.


# Problem
A discriminator should be used to classify the tokens predicted by the encoder, and a mask should be used as the input of the decoder for the tokens that are judged to be incorrectly predicted by the discriminator. However, in the initial stage of training, the discriminator prediction is inaccurate, which leads to mask token errors. Therefore, it is necessary to design an appropriate training strategy which can refer to the training strategy in the article Learning to Rewrite for Non-Autoregressive Neural Machine Translation.


# Usage
How to preprocess corpus, train model and inference, please refer to NATbase repository.
